storage: s3

# the name of AWS region where you're running cluster instances and where the bucket
# should be created
aws:
  s3_region: s3://eu-west-1
  # the name fo the bucket to store loki's data
  bucketnames: my-bucket-name

giantswarm:
  nodepool_id: my-nodepool-id
  ingress_name: &ingress_name loki.mycluster.k8s.gauss.eu-west-1.aws.gigantic.io
  s3_role: &s3_role my-s3-accesrole

# by default, the k8s' DNS is named `kube-dns`; you might override it here
# (please consult your cluster configuration for the correct value)
global:
  dnsService: "coredns"
  clusterDomain: "eu-west-1.local"

rbac:
  pspEnabled: true

# By default, Loki offers no authentication. It just expects the `X-Scope-OrgID`
# HTTP header to be set to indicate which tenant's logs are these. To enable
# minimal security, you can configure the gateway to route traffic through loki-multi-tenant-proxy
# to do HTTP basicAuth and overwriting the tenant id header
multiTenantAuth:
  enabled: true
  replicas: 3
  credentials: |-
    users:
      - username: Tenant1
        password: 1tnaneT
        orgid: tenant-1
      - username: Tenant2
        password: 2tnaneT
        orgid: tenant-2

# Loki consists of multiple microservices. Gateway is an nginx-based proxy
# that routes requests to correct microservices. It should be consider the entry point
# of your Loki deployment.
gateway:
  replicas: 3
  ingress:
    enabled: true
    annotations:
      # this annotation means cert-manager will automatically create a ACME certificate
      cert-manager.io/cluster-issuer: letsencrypt-giantswarm
    hosts:
      # host name assigned to your loki instance (must be registered in DNS)
      - host: *ingress_name
        paths:
          - /
    tls:
      - hosts:
          # host name assigned to your loki instance (must be registered in DNS)
          - *ingress_name
        secretName: loki-ingress-cert
  nginxConfig:
    file: |
      worker_processes  5;  ## Default: 1
      error_log  /dev/stderr;
      pid        /tmp/nginx.pid;
      worker_rlimit_nofile 8192;

      events {
        worker_connections  4096;  ## Default: 1024
      }

      http {
        client_body_temp_path /tmp/client_temp;
        proxy_temp_path       /tmp/proxy_temp_path;
        fastcgi_temp_path     /tmp/fastcgi_temp;
        uwsgi_temp_path       /tmp/uwsgi_temp;
        scgi_temp_path        /tmp/scgi_temp;

        default_type application/octet-stream;
        log_format   {{ .Values.gateway.nginxConfig.logFormat }}
        access_log   /dev/stderr  main;
        sendfile     on;
        tcp_nopush   on;
        resolver {{ .Values.global.dnsService }}.{{ .Values.global.dnsNamespace }}.svc.{{ .Values.global.clusterDomain }};

        {{- with .Values.gateway.nginxConfig.httpSnippet }}
        {{ . | nindent 2 }}
        {{- end }}

        server {
          listen             8080;

          {{- if .Values.gateway.basicAuth.enabled }}
          auth_basic           "Loki";
          auth_basic_user_file /etc/nginx/secrets/.htpasswd;
          {{- end }}

          location = / {
            return 200 'OK';
            auth_basic off;
          }

          location = /api/prom/push {
            proxy_pass       http://loki-multi-tenant-proxy.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3101$request_uri;
          }

          location = /api/prom/tail {
            proxy_pass       http://loki-multi-tenant-proxy.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
          }

          location ~ /api/prom/.* {
            proxy_pass       http://loki-multi-tenant-proxy.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3102$request_uri;
          }

          location = /loki/api/v1/push {
            proxy_pass       http://loki-multi-tenant-proxy.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3101$request_uri;
          }

          location = /loki/api/v1/tail {
            proxy_pass       http://loki-multi-tenant-proxy.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
          }

          location ~ /loki/api/.* {
            proxy_pass       http://loki-multi-tenant-proxy.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3102$request_uri;
          }

          {{- with .Values.gateway.nginxConfig.serverSnippet }}
          {{ . | nindent 4 }}
          {{- end }}
        }
      }

# Shared configuration for all components
# key _shared_config does not actually exist in the apps values.yaml
# We're using it in other keys through yaml anchors
_shared_config: &shared-conf
  podAnnotations:
    # The annotation below is `kiam` specific
    # and means that pods having it can use the IAM Role for S3 access
    iam.amazonaws.com/role: *s3_role
  persistence:
    storageClass: gp3

compactor: *shared-conf

ruler: *shared-conf

ingester:
  <<: *shared-conf
  replicas: 3
  affinity: |
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: giantswarm.io/machine-deployment
            operator: In
            values:
            - {{ .Values.giantswarm.nodepool_id }}
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              {{- include "loki.ingesterSelectorLabels" . | nindent 10 }}
          topologyKey: kubernetes.io/hostname
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                {{- include "loki.ingesterSelectorLabels" . | nindent 12 }}
            topologyKey: failure-domain.beta.kubernetes.io/zone

querier:
  <<: *shared-conf
  replicas: 2
  affinity: |
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: giantswarm.io/machine-deployment
            operator: In
            values:
            - {{ .Values.giantswarm.nodepool_id }}
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              {{- include "loki.querierSelectorLabels" . | nindent 10 }}
          topologyKey: kubernetes.io/hostname
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                {{- include "loki.querierSelectorLabels" . | nindent 12 }}
            topologyKey: failure-domain.beta.kubernetes.io/zone
